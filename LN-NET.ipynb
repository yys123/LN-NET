{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e013dd47-28f6-474c-b981-1c794cd1e2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e660bdb8-d5b2-4271-b0fb-64c7c64ecf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import copy\n",
    "import scipy.sparse as sp_sparse\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df2dcb0c-4437-4d2a-a314-44f1f9f771e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "iterations=100000\n",
    "alpha=0.01\n",
    "lambd=0.0000001\n",
    "maxBreak=10\n",
    "minImprovement=0.8\n",
    "minibatch=1000\n",
    "dropout_ratio=1.0\n",
    "threads=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2811186c-5264-44a3-985d-a610ebe02d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Loading data--------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------Loading data--------------------------\")\n",
    "\n",
    "#Load data\n",
    "inputDATA=\"/home/data/t0202002/SLENN/SLE/GeneData.csv\"\n",
    "ClassLabelDATA=\"/home/data/t0202002/SLENN/SLE/ClassLabels.csv\"\n",
    "# EdgeDATA=\"/home/data/t0202002/SLENN/SLE/Edgelist.csv\"\n",
    "EdgeDATA=\"/home/data/t0202002/SLENN/SLE/Edgelist.csv\"\n",
    "outputPath=\"/home/data/t0202002/SLENN/SLE/results/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e06bb225-5fe2-4aa1-9e65-bd1f4815803b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(outputPath):\n",
    "    os.mkdir(outputPath)\n",
    "i = 1\n",
    "while os.path.exists(os.path.join(outputPath, \"Run_\" + str(i))):\n",
    "    i += 1\n",
    "outputPath = os.path.join(outputPath, \"Run_\" + str(i))\n",
    "os.mkdir(outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49569736-a19a-4d3d-b630-755d5419d151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene expression DATA --------------------------------------------------------------------------------------------------------------------\n",
    "df=pd.read_csv(inputDATA, sep=\",\", index_col=0)\n",
    "genelist_sc = df.index.tolist()\n",
    "cell_sc = df.columns.tolist()\n",
    "alldata_matrix = sp_sparse.csc_matrix(df.values.astype(\"float64\"))\n",
    "genelist_sc = [x + \"_gene\" for x in genelist_sc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd24052d-99d7-4ca4-b107-ffe84af29766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(alldata_matrix[1:5,1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4b00b2a-8f2f-4378-a421-d696fbdc6e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classLabelDATA \n",
    "def index_match(list2, list1):\n",
    "    df={}\n",
    "    result=[]\n",
    "    for idx, x in enumerate(list1):\n",
    "        df[x]=idx\n",
    "    for idx, x in enumerate(list2):\n",
    "        result.append(df[x])\n",
    "    return(result)\n",
    "    \n",
    "label_data=pd.read_csv(ClassLabelDATA, sep=\",\")\n",
    "cell_y = label_data[\"barcode\"].tolist()\n",
    "barcodes = sorted(set(cell_y) & set(cell_sc))\n",
    "label_data=label_data.loc[index_match(barcodes, cell_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d22f7458-f621-4d7a-9ee7-0b9c81706e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "for i in range(alldata_matrix.shape[1]):\n",
    "    x=alldata_matrix.data[alldata_matrix.indptr[i]:alldata_matrix.indptr[i+1]]\n",
    "    alldata_matrix.data[alldata_matrix.indptr[i]:alldata_matrix.indptr[i+1]]=x/x.sum()*1e6\n",
    "alldata_matrix = alldata_matrix.log1p() \n",
    "alldata_matrix = alldata_matrix[index_match(genesList, genelist_sc),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b07ea6e0-6760-4e70-ba8b-e8299143e0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: test: 200 val: 200 train: 600 of:  1000\n",
      "1: test: 200 val: 200 train: 600 of:  1000\n"
     ]
    }
   ],
   "source": [
    "# Generate train and test data\n",
    "test_idx = []\n",
    "val_idx = []\n",
    "train_idx = []\n",
    "\n",
    "if len(test_idx) + len(val_idx) + len(train_idx) != len(barcodes):\n",
    "    nrTestCells = int(len(barcodes)*0.2) #20% for test\n",
    "    test_idx = []\n",
    "    val_idx = []\n",
    "    train_idx = []\n",
    "    maxCells=999999\n",
    "    if nrTestCells > maxCells: nrTestCells = maxCells    \n",
    "    test_groups = [\"\".join([str(i) for i in x]) for x in np.transpose(full_data.astype(\"int\")).tolist()]\n",
    "    test_groups_list={}\n",
    "    for test_grp in set(test_groups):\n",
    "        test_groups_list[test_grp] = []\n",
    "    for idx, x in enumerate(test_groups):\n",
    "        if not idx in test_idx: \n",
    "            test_groups_list[x].append(idx)\n",
    "    for test_grp in sorted(set(test_groups)):\n",
    "        test_and_val_group_N = int(((len(test_groups_list[test_grp]) + 0.0)/len(test_groups)) * nrTestCells * 2)\n",
    "        test_and_val_barcodes=np.random.choice(test_groups_list[test_grp], test_and_val_group_N, replace=False).tolist()\n",
    "        val_idx_x = np.random.choice(test_and_val_barcodes, int(test_and_val_group_N/2), replace=False).tolist()\n",
    "        test_idx_x = sorted(set(test_and_val_barcodes) - set(val_idx_x)) \n",
    "        train_idx_x = sorted(set(test_groups_list[test_grp]) - set(val_idx_x) - set(test_idx_x))\n",
    "        print(test_grp + \": test: \" + str(len(test_idx_x)) + \" val: \" + str(len(val_idx_x)) + \" train: \" + str(len(train_idx_x)) + \" of:  \" + str(len(test_groups_list[test_grp])))\n",
    "        test_idx = test_idx + test_idx_x\n",
    "        val_idx = val_idx + val_idx_x\n",
    "        train_idx = train_idx + train_idx_x\n",
    "y_train = full_data[:,train_idx]\n",
    "valdata_y = full_data[:,val_idx]\n",
    "testdata_y = full_data[:,test_idx]\n",
    "x_train = alldata_matrix[:,train_idx]\n",
    "valdata_x = alldata_matrix[:,val_idx]\n",
    "testdata_x = alldata_matrix[:,test_idx]\n",
    "\n",
    "x_train = x_train.tocsr()\n",
    "valdata_x = valdata_x.tocsr()\n",
    "testdata_x = testdata_x.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e07f5e96-c33b-44ce-b14b-7c8061b1f261",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(alldata_matrix.shape[0]):\n",
    "    x_train_row=x_train.data[x_train.indptr[i]:x_train.indptr[i+1]]\n",
    "    valdata_x_row=valdata_x.data[valdata_x.indptr[i]:valdata_x.indptr[i+1]]\n",
    "    testdata_x_row=testdata_x.data[testdata_x.indptr[i]:testdata_x.indptr[i+1]]\n",
    "    if x_train_row.shape[0] == x_train.shape[1]:\n",
    "        x_train_row_min=x_train_row.min()\n",
    "        x_train_row=x_train_row-x_train_row_min\n",
    "        valdata_x_row=valdata_x_row-x_train_row_min\n",
    "        testdata_x_row=testdata_x_row-x_train_row_min\n",
    "    \n",
    "    if x_train_row.shape[0] > 0:    \n",
    "        x_train_row_max = x_train_row.max()\n",
    "        x_train_row=x_train_row/x_train_row_max\n",
    "        valdata_x_row=valdata_x_row/x_train_row_max\n",
    "        testdata_x_row=testdata_x_row/x_train_row_max\n",
    "    else:\n",
    "        if valdata_x_row.shape[0] > 0:   \n",
    "            if valdata_x_row.shape[0] == valdata_x.shape[1]: \n",
    "                valdata_x_row=valdata_x_row-valdata_x_row.min()\n",
    "            valdata_x_row=valdata_x_row/valdata_x_row.max()\n",
    "        if testdata_x_row.shape[0] > 0:   \n",
    "            if testdata_x_row.shape[0] == testdata_x.shape[1]: \n",
    "                testdata_x_row=testdata_x_row-testdata_x_row.min()\n",
    "            testdata_x_row=testdata_x_row/testdata_x_row.max()\n",
    "    \n",
    "    x_train.data[x_train.indptr[i]:x_train.indptr[i+1]]=x_train_row\n",
    "    valdata_x.data[valdata_x.indptr[i]:valdata_x.indptr[i+1]]=valdata_x_row\n",
    "    testdata_x.data[testdata_x.indptr[i]:testdata_x.indptr[i+1]]=testdata_x_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c57a3440-f609-4ba5-aaf3-14168da37af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-entropy loss of each training sample was weighted by the following equation:\n",
    "def weight_eq(ys_input):\n",
    "    matrix_groups = [\"\".join([str(i) for i in x]) for x in np.transpose(ys_input.astype(\"int\")).tolist()]\n",
    "    matrix_groups_weight={}\n",
    "    beta=0.01\n",
    "    for matrix_grp in sorted(set(matrix_groups)):\n",
    "        matrix_groups_weight[matrix_grp] = (1.0/((matrix_groups.count(matrix_grp) + beta) / len(matrix_groups))) / len(set(matrix_groups))\n",
    "        #print(matrix_groups_weight[matrix_grp])\n",
    "    #print(matrix_groups_weight)    \n",
    "    return(np.array([[matrix_groups_weight[x] for x in matrix_groups],]* ys_input.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9009b555-fa6b-4423-b727-ab1cd4435754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing data \n",
    "valdata_x = valdata_x.toarray() \n",
    "valdata_y_weights = weight_eq(valdata_y)\n",
    "testdata_x = testdata_x.toarray()\n",
    "testdata_y_weights = weight_eq(testdata_y)\n",
    "\n",
    "# Shuffle data \n",
    "rd.shuffle(genesList)\n",
    "\n",
    "if minibatch == 0 or minibatch > x_train.shape[1]:\n",
    "    minibatch = x_train.shape[1]\n",
    "else:\n",
    "    minibatch = int(x_train.shape[1]//(x_train.shape[1]//(minibatch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11d1dc1d-1132-415b-b0c3-daa9b13f6d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get edge list\n",
    "edges={}\n",
    "for index, row in pathway_data.iterrows():\n",
    "    if(row['parent'] in edges.keys()):\n",
    "        edges[row['parent']] = edges[row['parent']] + [row['child']]\n",
    "    else:\n",
    "        edges[row['parent']] = [row['child']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bce038b2-3aff-4408-b8be-09bdf6af372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "GeneMap = {}   \n",
    "NodeMap = {}   \n",
    "weightMap = {}    \n",
    "ids = 0\n",
    "for iNode in nodesRanks:\n",
    "    GeneMap[iNode] = []\n",
    "    NodeMap[iNode] = []\n",
    "    weightMap[iNode]=[]\n",
    "    for egde in edges[iNode]:\n",
    "        weightMap[iNode].append(ids)\n",
    "        ids += 1\n",
    "        if egde in genesList:\n",
    "            GeneMap[iNode].append(genesList.index(egde))\n",
    "        elif egde in nodesRanks:\n",
    "            NodeMap[iNode].append(nodesRanks.index(egde))\n",
    "        else:\n",
    "            print(egde,\"was not found\")\n",
    "\n",
    "weightTotalLength = sum([len(weightMap[i]) for i in nodesRanks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d00e61fe-bec6-4647-8a13-d742c1f5a184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout ratio \n",
    "dropoutKP_GENES = tf.cast(tf.compat.v1.placeholder_with_default(input=1.0, shape=[]), tf.float64)\n",
    "\n",
    "output_true = tf.compat.v1.placeholder(shape=[1,None],dtype=tf.float64)\n",
    "output_weights = tf.compat.v1.placeholder(shape=[1,None],dtype=tf.float64)\n",
    "\n",
    "# Genes \n",
    "genesOrig = tf.compat.v1.placeholder(shape=[x_train.shape[0], None],dtype=tf.float64)\n",
    "if dropout_ratio < 1:\n",
    "    genes = tf.nn.dropout(x=genesOrig, rate=1 - (dropoutKP_GENES))\n",
    "else:\n",
    "    genes = genesOrig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "627bf146-5a66-4baf-a571-988ec8e4d4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/data/t0202002/miniconda3/envs/slenn/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'histogram_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numApproxPlus = tf.compat.v1.placeholder_with_default(input=\"\", shape=[])\n",
    "numApproxMinus = tf.compat.v1.placeholder_with_default(input=\"\", shape=[])\n",
    "numGradEpsilon=0.001\n",
    "numApproxVec = tf.reshape(tf.cast(tf.tile([numGradEpsilon], [tf.shape(input=genesOrig)[1]],), tf.float64), [1, tf.shape(input=genesOrig)[1]])\n",
    "\n",
    "# Edge weights\n",
    "weights = tf.Variable(tf.random.normal([weightTotalLength,1], dtype=tf.float64), dtype=tf.float64)\n",
    "tf.compat.v1.summary.scalar(\"mean\", tf.reduce_mean(input_tensor=weights))\n",
    "tf.compat.v1.summary.histogram(\"histogram\", weights)\n",
    "\n",
    "\n",
    "interceptWeights = tf.Variable(tf.random.normal([len(nodesRanks)], dtype=tf.float64), dtype=tf.float64)\n",
    "tf.compat.v1.summary.scalar(\"mean\", tf.reduce_mean(input_tensor=interceptWeights))\n",
    "tf.compat.v1.summary.histogram(\"histogram\", interceptWeights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a4469b-8e21-4a74-a8be-1cf403edf4f0",
   "metadata": {},
   "source": [
    "idxs = list(range(x_train.shape[1]))\n",
    "np.random.shuffle(idxs)\n",
    "minibatch_list = [idxs[j:min(j+minibatch, len(idxs)-1)] for j in list(range(0, x_train.shape[1], minibatch))]\n",
    "minibatch_list_length = [len(x) for x in minibatch_list]\n",
    "print(len(idxs),\"idxs\")\n",
    "\n",
    "for i_batch in range(len(minibatch_list)):\n",
    "    print(i_batch,\"i_batch\")\n",
    "    batch_idx=minibatch_list[i_batch]\n",
    "    x_batch=x_train[:,batch_idx].toarray()\n",
    "    with tf.Session() as sess:\n",
    "        a=sess.run(genesOrig,{genesOrig:x_batch})\n",
    "        print(a,a.shape)\n",
    "        b=sess.run(numApproxVec,{genesOrig:x_batch})\n",
    "        print(b,b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430081bf-5978-49de-b8f1-76e46ce0f165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout nodes\n",
    "nodes = {}\n",
    "nodes_loss = {}\n",
    "genes_unstacked = tf.unstack(genes)\n",
    "for n in nodesRanks:\n",
    "    weightsX = tf.slice(weights, [weightMap[n][0],0],[len(weightMap[n]),1])\n",
    "    features = tf.stack([genes_unstacked[x] for x in GeneMap[n]] + [nodes[nodesRanks[nidx]] for nidx in NodeMap[n]])\n",
    "    z = tf.matmul(tf.transpose(a=weightsX), features) \n",
    "    z = z + tf.slice(interceptWeights, [nodesRanks.index(n)],[1])\n",
    "    if n==\"output\":\n",
    "        nodes[n] = z\n",
    "    else:\n",
    "        nodes[n] = tf.nn.sigmoid(z)        \n",
    "\n",
    "    nodes[n] = tf.cond(pred=tf.equal(numApproxPlus, tf.constant(n)), true_fn=lambda: nodes[n] + numApproxVec, false_fn=lambda: nodes[n])\n",
    "    nodes[n] = tf.cond(pred=tf.equal(numApproxMinus, tf.constant(n)), true_fn=lambda: nodes[n] - numApproxVec, false_fn=lambda: nodes[n])\n",
    "\n",
    "    nodes[n] = tf.unstack(nodes[n])[0]\n",
    "    # print(n, nodes[n])\n",
    "    \n",
    "    \n",
    "    if not n in outputs:\n",
    "        parents = pathway_data[pathway_data['child'].str.match(\"^\" + n + \"$\")]['parent'].tolist()\n",
    "        children = len(set(pathway_data[pathway_data['parent'].isin(parents)][\"child\"].tolist()))\n",
    "        #print(parents,children)\n",
    "        if children == 1:\n",
    "            print(\"Skipping dropout for \" + n)\n",
    "        else:\n",
    "            nodes[n] = tf.nn.dropout(x=nodes[n], rate=1 - dropout_ratio)\n",
    "            # print(nodes[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5da42dc5-f64b-46dd-8e1d-e8960d1984e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation results\n",
    "weightsToPenalize = weights\n",
    "regularization = (lambd * tf.nn.l2_loss(weightsToPenalize)) / tf.cast(tf.shape(input=output_true)[1], tf.float64)\n",
    "# tf.compat.v1.summary.scalar(\"value\", regularization)\n",
    "\n",
    "xentropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=tf.stack([nodes[x] for x in outputs]), labels=output_true) * output_weights\n",
    "# tf.compat.v1.summary.scalar(\"mean\", tf.reduce_mean(input_tensor=xentropy))\n",
    "# tf.compat.v1.summary.histogram(\"histogram\", xentropy)\n",
    "\n",
    "loss = tf.reduce_mean(input_tensor=xentropy) + regularization\n",
    "# tf.compat.v1.summary.scalar(\"value\", loss)\n",
    "\n",
    "y_hat = tf.nn.sigmoid(tf.stack([nodes[x] for x in outputs]))\n",
    "\n",
    "error = tf.abs(output_true - y_hat) * output_weights\n",
    "# tf.compat.v1.summary.scalar(\"mean\", tf.reduce_mean(input_tensor=error))\n",
    "\n",
    "accuracy = tf.reduce_mean(input_tensor=tf.cast(tf.equal(tf.round(y_hat), output_true), tf.float64))\n",
    "# tf.compat.v1.summary.scalar(\"mean\", tf.reduce_mean(input_tensor=accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd380fdc-5c05-4fce-aa1a-3d044819d4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdamOptimizer\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=alpha)    \n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c6032f3-54d3-4ff8-8ce7-f602cbfc89e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "...Initializing\n"
     ]
    }
   ],
   "source": [
    "# model Initialization\n",
    "print(\"----------------\\n...Initializing\")\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(inter_op_parallelism_threads=threads,intra_op_parallelism_threads=threads))\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "merged = tf.compat.v1.summary.merge_all()\n",
    "saver = tf.compat.v1.train.Saver()\n",
    "sep=\",\"\n",
    "\n",
    "costFile = open(os.path.join(outputPath, \"performance.csv\"),\"w\")\n",
    "costFile.write(\"iteration,train,validation,error,accuracy,BreakCount\" + \"\\n\")\n",
    "costFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cac51b1-5476-4780-9e1b-14684709832d",
   "metadata": {},
   "source": [
    "idxs = list(range(x_train.shape[1]))\n",
    "np.random.shuffle(idxs)\n",
    "minibatch_list = [idxs[j:min(j+minibatch, len(idxs)-1)] for j in list(range(0, x_train.shape[1], minibatch))]\n",
    "minibatch_list_length = [len(x) for x in minibatch_list]\n",
    "print(len(idxs),\"idxs\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i_batch in range(len(minibatch_list)):\n",
    "        print(i_batch,\"i_batch\")\n",
    "        batch_idx=minibatch_list[i_batch]\n",
    "        x_batch=x_train[:,batch_idx].toarray()\n",
    "        y_batch=y_train[:,batch_idx]\n",
    "        y_batch_weights = weight_eq(y_batch)\n",
    "        lossRes, trainClassProb, trainWriteContent = sess.run([loss, y_hat, merged], {genesOrig:x_batch, output_true:y_batch, dropoutKP_GENES: dropout_ratio, output_weights:y_batch_weights})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f3e422-697d-4fb6-ad39-798085bcac0b",
   "metadata": {},
   "source": [
    "batch_idx=minibatch_list[0]\n",
    "x_batch=x_train[:,batch_idx]\n",
    "y_batch=y_train[:,batch_idx]\n",
    "print(type(y_batch),\"什么\")\n",
    "y_batch=y_train[:,batch_idx]\n",
    "y_batch_weights = weight_eq(y_batch)\n",
    "x_batch,y_batch,y_batch_weights,x_batch.shape,y_batch.shape,y_batch_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28ce940e-2c82-48b5-9e1c-eb919e0d49ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3504, 400)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5363cdc7-27b5-485e-b264-d278f0b1055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "trainCost = []\n",
    "valErr = []\n",
    "valErrCurrentMin = 1\n",
    "BreakCount = 0\n",
    "\n",
    "for i in [xx + 1 for xx in range(iterations)]:    \n",
    "    # DEFINE MINIBATCH    \n",
    "    idxs = list(range(x_train.shape[1]))\n",
    "    np.random.shuffle(idxs)\n",
    "    minibatch_list = [idxs[j:min(j+minibatch, len(idxs)-1)] for j in list(range(0, x_train.shape[1], minibatch))]\n",
    "    minibatch_list_length = [len(x) for x in minibatch_list]\n",
    "    if 0 in minibatch_list_length:\n",
    "        minibatch_list_idx_remove = minibatch_list_length.index(0)\n",
    "        del minibatch_list[minibatch_list_idx_remove]\n",
    "    for i_batch in range(len(minibatch_list)):\n",
    "        batch_idx=minibatch_list[i_batch]\n",
    "        x_batch=x_train[:,batch_idx].toarray()\n",
    "        y_batch=y_train[:,batch_idx]\n",
    "        y_batch_weights = weight_eq(y_batch)\n",
    "       dropout_ratio=dropout_ratio*N0/(N0+1)*(dropout_ratio** 0.5)\n",
    "        if i > 1: \n",
    "            sess.run(train, {genesOrig:x_batch, output_true:y_batch, dropoutKP_GENES:dropout_ratio, output_weights:y_batch_weights})\n",
    "\n",
    "    if i < 10 or i % 10 == 0:\n",
    "        print(\"\\nTraining epoch: \" + str(i))\n",
    "        # lossRes=sess.run(loss, {genesOrig:x_batch, output_true:y_batch, dropoutKP_GENES: dropout_ratio, output_weights:y_batch_weights})\n",
    "        lossRes, trainClassProb, trainWriteContent = sess.run([loss, y_hat, merged], {genesOrig:x_batch, output_true:y_batch, dropoutKP_GENES: dropout_ratio, output_weights:y_batch_weights})\n",
    "        lossValRes, valWriteContent, valerror, valAccuracy = sess.run([loss, merged, error,accuracy], {genesOrig:valdata_x, output_true:valdata_y, output_weights:valdata_y_weights})\n",
    "        valErrRun = valerror.mean()\n",
    "                \n",
    "        # Early stopping\n",
    "        if i > 10:\n",
    "            if trainCost[-1] - lossRes < 0.00001: \n",
    "                BreakCount += 1\n",
    "            if any(valErrPrevious + 0.05 < valErrRun for valErrPrevious in valErr): \n",
    "                BreakCount += 1\n",
    "      \n",
    "        # record the training progress\n",
    "        print(\"Mean loss: \" + \"%.4f\" % lossRes + \" Validation loss: \" + \"%.4f\" % lossValRes + \" Validation error: \" + str(valErrRun))\n",
    "        print(\"Break Counter = \" + str(BreakCount))\n",
    "        costFile = open(os.path.join(outputPath, \"performance.csv\"),\"a\")\n",
    "        costFile.write(str(i) + sep + str(lossRes) + sep + str(lossValRes) + sep + str(valErrRun) + sep + str(valAccuracy.mean()) + sep + str(BreakCount) + sep + str(dropout_ratio) + \"\\n\")\n",
    "        costFile.close()\n",
    "        \n",
    "        if BreakCount > maxBreak:\n",
    "            break\n",
    "        \n",
    "        # Save model\n",
    "        if i > 2 and (valErrRun < 0.2 and valErrCurrentMin * minImprovement > valErrRun):\n",
    "            BreakCount = 0\n",
    "            saver.save(sess, os.path.join(outputPath,'Best-model'))\n",
    "            valErrCurrentMin = valErrRun\n",
    "            print(\"Model saved at iteration \" + str(i) + \" at Validation error: \" + str(valErrRun))\n",
    "        saver.save(sess, os.path.join(outputPath,'Best-model'))\n",
    "        trainCost.append(lossRes)\n",
    "        valErr.append(valErrRun)\n",
    "        \n",
    "# Load the best model\n",
    "if os.path.exists(os.path.join(outputPath,'Best-model.meta')):\n",
    "    saver.restore(sess, os.path.join(outputPath,'Best-model'))\n",
    "    print(\"Model restored from \" + outputPath)\n",
    "\n",
    "ValidationError = sess.run(error, {genesOrig:testdata_x, output_true:testdata_y, output_weights:testdata_y_weights}).mean()\n",
    "print(\"ValidationError:\" + str(ValidationError))\n",
    "open(os.path.join(outputPath, \"ValidationError.txt\"),\"w\").write(\"ValidationError\" + \"\\n\" + str(ValidationError))\n",
    "\n",
    "# record predict results\n",
    "output_predict = sess.run(y_hat, {genesOrig:testdata_x, output_true:testdata_y})\n",
    "output_predict_file = open(os.path.join(outputPath, \"output_predict.csv\"),\"w\")\n",
    "output_predict_file.write(\",\".join(outputs) + \"\\n\")\n",
    "output_predict_file.write(pd.DataFrame(np.transpose(output_predict)).to_csv(header=False, index=False))\n",
    "output_predict_file.close()\n",
    "# real data\n",
    "output_real_file = open(os.path.join(outputPath, \"output_real.csv\"),\"w\")\n",
    "output_real_file.write(\",\".join(outputs) + \"\\n\")\n",
    "output_real_file.write(pd.DataFrame(np.transpose(testdata_y.astype(\"int\"))).to_csv(header=False, index=False))\n",
    "output_real_file.close()\n",
    "\n",
    "# Weight output\n",
    "sep = \",\"\n",
    "weightRes, interceptRes = sess.run([weights, interceptWeights])\n",
    "weightFile = open(os.path.join(outputPath, \"Edge_weights.csv\"),\"w\")\n",
    "weightFile.write(\"parent,child,weight\" + \"\\n\")\n",
    "for n in nodesRanks:\n",
    "    i = 0\n",
    "    for targetGene in GeneMap[n]:\n",
    "        weightFile.write(n + sep + genesList[targetGene] + sep + str(weightRes[weightMap[n][i]].tolist()[0]) + \"\\n\")\n",
    "        i += 1\n",
    "    for targetProtein in NodeMap[n]:\n",
    "        weightFile.write(n + sep + nodesRanks[targetProtein] + sep + str(weightRes[weightMap[n][i]].tolist()[0]) + \"\\n\")\n",
    "        i += 1\n",
    "    assert i == len(weightMap[n])\n",
    "    weightFile.write(n + sep + \"Intercept\" + sep + str(interceptRes[nodesRanks.index(n)]) + \"\\n\")\n",
    "\n",
    "weightFile.close()\n",
    "\n",
    "\n",
    "print(\"Calculate contribution scores-------------------------------\")\n",
    "\n",
    "# calculate contribution scores \n",
    "numAgg = 0\n",
    "nodesNumGrad = [x for x in nodesRanks if not x in outputs]\n",
    "for n in nodesNumGrad:\n",
    "    res = (sess.run(y_hat, {genesOrig:testdata_x, output_true:testdata_y, numApproxPlus: n}) - sess.run(y_hat, {genesOrig:testdata_x, output_true:testdata_y, numApproxMinus: n}))/(2*numGradEpsilon)\n",
    "    res = res.mean(1)\n",
    "    if numAgg.__class__ != np.ndarray:\n",
    "        numAgg = res\n",
    "    else:\n",
    "        numAgg = np.column_stack((numAgg, res))\n",
    "\n",
    "# save contribution scores results\n",
    "sep = \",\"\n",
    "Contri_score = open(os.path.join(outputPath, \"Contri_score.csv\"),\"w\")\n",
    "Contri_score.write(\"Gene\" + sep + sep.join(outputs) + \"\\n\")\n",
    "for i,n in enumerate(nodesNumGrad):\n",
    "    Contri_score.write(n + sep + sep.join([str(x) for x in numAgg[:,i].tolist()]) + \"\\n\")\n",
    "\n",
    "Contri_score.close()\n",
    "\n",
    "sess.close()\n",
    "\n",
    "print(\"Model was successfully completed-------------------------------\")\n",
    "print(\"\\nResults are exported to \" + outputPath +\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08d6465-4d96-4144-be30-e39f14d1ed94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bba4c1a-15ba-4ad2-b4d2-9d486ac06446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04192b54-45b0-4749-8cbf-cca7a3e6673a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
